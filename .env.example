# =============================================================================
# ArXiv Database Project - Environment Configuration
# =============================================================================
# This file is for TESTING and LEARNING purposes only.
# DO NOT use in production environments without proper security review.
# Copy this file to .env and update with your actual configuration values.
# =============================================================================

# -----------------------------------------------------------------------------
# Database Configuration
# -----------------------------------------------------------------------------
# PostgreSQL database connection settings

# DB_HOST: Leave empty for Unix socket (peer auth), or use 'localhost' for TCP/IP
DB_HOST=
DB_PORT=5432
DB_NAME=arxiv
DB_USER=postgres
DB_PASSWORD=

# SQLAlchemy database URL (alternative to individual DB_* variables)
# Format: postgresql://user:password@host:port/database
# Note: Empty host uses Unix socket (peer authentication), 'localhost' uses TCP/IP (password required)
# For Unix socket: postgresql://user@/database
# For TCP/IP:      postgresql://user:password@localhost:5432/database
SQLALCHEMY_URL=postgresql://postgres@/arxiv

# IMPORTANT: Make sure DB_USER matches your PostgreSQL user
# For Unix socket connections (DB_HOST=''), the DB_USER must match your system username
# or you'll get "Peer authentication failed" errors.
# Example: If your system user is 'john', use DB_USER=john

# -----------------------------------------------------------------------------
# File Storage Paths
# -----------------------------------------------------------------------------
# Path to the arXiv metadata JSON file
# Download from: https://www.kaggle.com/datasets/Cornell-University/arxiv
ARXIV_DATA_PATH=./arxiv-metadata-oai-snapshot.json

# Base directory for storing PDF files (LEGACY - for old scripts only)
# NOTE: The new unified script (process_arxiv.py) stores PDFs directly in PostgreSQL
# as binary data (BYTEA). This path is only used by legacy scripts:
#   - download_pdfs.py (downloads to filesystem)
#   - load_pdfs_to_db.py (loads from filesystem to database)
#   - sync_pdf_status.py (syncs filesystem with database flags)
# Recommended: Use the unified script instead for direct database storage
PDF_BASE_PATH=./arxiv_pdfs

# -----------------------------------------------------------------------------
# Download Configuration
# -----------------------------------------------------------------------------
# Delay between PDF downloads in seconds (respect arXiv rate limits)
# IMPORTANT: Please respect arXiv's terms of service
# - Minimum recommended: 3.0 seconds
# - Default: 3.0 seconds
# - Used by: process_arxiv.py (unified script) and download_pdfs.py (legacy)
DOWNLOAD_DELAY=3.0

# -----------------------------------------------------------------------------
# Processing Configuration
# -----------------------------------------------------------------------------
# Number of CPU cores to use for parallel processing
# Used by: fast_analysis.py for metadata analysis
# Set to 0 to use all available cores, or specify a number (e.g., 4, 8, 16)
CPU_COUNT=4

# -----------------------------------------------------------------------------
# New Unified Script (process_arxiv.py)
# -----------------------------------------------------------------------------
# The new unified script combines metadata loading and PDF downloading into
# a single pipeline. It uses these environment variables:
#
# Required:
#   - SQLALCHEMY_URL (or DB_HOST, DB_PORT, DB_NAME, DB_USER, DB_PASSWORD)
#   - ARXIV_DATA_PATH (path to metadata JSONL file)
#
# Optional:
#   - DOWNLOAD_DELAY (default: 3.0 seconds)
#
# NOT used (PDFs stored in database):
#   - PDF_BASE_PATH (not needed for unified script)
#
# Example usage:
#   python scripts/process_arxiv.py --limit 5000 --workers 4
#
# Features:
#   - Automatic resume (skips papers already in database)
#   - Concurrent downloads (configurable workers)
#   - PDFs stored directly in PostgreSQL (BYTEA column)
#   - Rate limiting with configurable delays
#   - Error tracking and retry support
